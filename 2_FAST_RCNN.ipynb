{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make R-CNN faster, Girshick (2015) improved the training procedure by unifying three independent models into one jointly trained framework and increasing shared computation results, named Fast R-CNN. Instead of extracting CNN feature vectors independently for each region proposal, this model aggregates them into one CNN forward pass over the entire image and the region proposals share this feature matrix. Then the same feature matrix is branched out to be used for learning the object classifier and the bounding-box regressor. In conclusion, computation sharing speeds up R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#First, pre-train a convolutional neural network on image classification tasks.\n",
    "#Propose regions by selective search (~2k candidates per image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from dataload import xml_to_csv,PetData,Sub_region_train,Sub_region\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utills import ssearch,misc\n",
    "from utills.misc import create_label,balance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading data\n",
    "#root_path=\"D:/Dataset/Pet_Data/\"\n",
    "root_path=\"D:/Dataset/Pet_Data/\"\n",
    "#root_path=\"D:/Dataset/Oxford\"\n",
    "\n",
    "img_path=os.path.join(root_path,\"images\")\n",
    "annotation_path=os.path.join(root_path,\"annotations/xmls\")           \n",
    "annots = glob.glob(annotation_path+\"/*.xml\")\n",
    "seed=0\n",
    "df=xml_to_csv(annots,img_path)\n",
    "df.head()\n",
    "\n",
    "## Make Balanced Dataset (To save time, but don't do this in real research!!)\n",
    "g = df.groupby('target')\n",
    "balanced_df = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min(),random_state=seed).reset_index(drop=True),))  \n",
    "## \n",
    "train, valid = train_test_split(balanced_df, test_size=0.3,random_state=seed)  \n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Dataset Class (Added Selective Search \"inside\" original class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetData(Dataset):\n",
    "    def __init__(self, dataframe,train=False,ssearch=False):\n",
    "        self.df=dataframe\n",
    "        self.ssearch=ssearch\n",
    "        self.transform=iaa.Sequential([iaa.Resize((512,512))])\n",
    "        self.torch_transform=T.Compose([T.ToTensor(),\n",
    "                                        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])                         \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        regions=None\n",
    "        fn,target,xmin,ymin,xmax,ymax=self.df.iloc[idx] #\n",
    "        im=cv2.cvtColor(cv2.imread(fn),cv2.COLOR_BGR2RGB) ##Load Img\n",
    "        \n",
    "        class_label=target  ##Class\n",
    "        bbs=BoundingBoxesOnImage([BoundingBox(xmin,ymin,xmax,ymax,label=class_label)], shape=im.shape) #BBox\n",
    "        image_aug, bbs_aug = self.transform(image=im, bounding_boxes=bbs) #Transformation\n",
    "        \n",
    "                                        \n",
    "        if self.ssearch:                                \n",
    "            regions=ssearch.selective_search(image_aug, scale=50, sigma=0.8, min_size=30)\n",
    "            regions=pd.DataFrame.from_dict(regions)\n",
    "            regions=regions.drop_duplicates(subset=['rect'])\n",
    "\n",
    "        return self.torch_transform(image_aug), torch.stack([torch.tensor([bb.x1,bb.y1,bb.x2,bb.y2,bb.label]) for bb in bbs_aug]),regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PetData(train, ssearch=True)\n",
    "valid_ds= PetData(valid, ssearch=True)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return zip(*batch)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE\n",
    "                                       , collate_fn=collate_fn,shuffle=False)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE\n",
    "                                       , collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds=train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>    First, pre-train a convolutional neural network on image classification tasks. </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\HyunSeung/.cache\\torch\\checkpoints\\vgg16-397923af.pth\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 528M/528M [00:09<00:00, 60.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of parameters: 138357544\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                              \n",
    "print(f'Num of parameters: {count_parameters(model)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>    We replace Last Maxpooling with ROI pooling. </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= model.features[0:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class roi_pooling(nn.Module):\n",
    "    def __init__(self,ft_shape=32,img_shape=512):\n",
    "        '''\n",
    "        '''\n",
    "        super(roi_pooling, self).__init__()\n",
    "    \n",
    "    \n",
    "        #Layers\n",
    "        self.img_shape=img_shape\n",
    "        self.ft_shape=ft_shape\n",
    "        self.adaptivepool=nn.AdaptiveMaxPool2d(size[0], size[1])\n",
    "\n",
    "    def forward(self,ft,region_box):\n",
    "        \n",
    "        scaled_bbox=[0]*4\n",
    "        scaled_bbox[0:4]=region_box[0:4]/self.img_shape\n",
    "        scaled_bbox=[int(x*self.ft_shape) for x in scaled_bbox]\n",
    "        roi=ft[:,:,scaled_bbox[1]:scaled_bbox[3],scaled_bbox[0]:scaled_bbox[2]]\n",
    "        roi=self.adaptivepool(roi)[0]\n",
    "        return roi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fast_RCNN(nn.Module):\n",
    "    def __init__(self,ft_shape=32,img_shape=512):\n",
    "        '''\n",
    "        '''\n",
    "        super(Fast_RCNN, self).__init__()\n",
    "    \n",
    "    \n",
    "        #Layers\n",
    "        self.ft_net=models.vgg16(pretrained=True).features[0:-1]\n",
    "        self.roi=roi_pooling()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,imgs,regions):\n",
    "        \n",
    "        scaled_bbox=[0]*4\n",
    "        scaled_bbox[0:4]=region_box[0:4]/self.img_shape\n",
    "        scaled_bbox=[int(x*self.ft_shape) for x in scaled_bbox]\n",
    "        roi=ft[:,:,scaled_bbox[1]:scaled_bbox[3],scaled_bbox[0]:scaled_bbox[2]]\n",
    "        roi=self.adaptivepool(roi)[0]\n",
    "        return roi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_pool=roi_pooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_pool(fts,bbox).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  0,   1,   2,  ...,   5,   6,   7],\n",
       "          [  9,  10,  11,  ...,  32,  15,  16],\n",
       "          [ 27,  28,  29,  ...,  32,  33,  34],\n",
       "          ...,\n",
       "          [ 73,  74,  74,  ...,  68,  69,  70],\n",
       "          [ 81,  82,  83,  ...,  86,  87,  88],\n",
       "          [ 99, 100, 101,  ..., 104, 105, 106]],\n",
       "\n",
       "         [[  0,   1,   2,  ...,   5,   6,   7],\n",
       "          [  9,  10,  11,  ...,  14,  15,  16],\n",
       "          [ 27,  28,  29,  ...,  32,  33,  34],\n",
       "          ...,\n",
       "          [ 82,  82,  65,  ...,  68,  69,  70],\n",
       "          [ 99,  82,  83,  ...,  86,  87,  88],\n",
       "          [ 99, 100, 101,  ..., 104, 105, 106]],\n",
       "\n",
       "         [[  0,   1,   2,  ...,   5,  16,  17],\n",
       "          [  9,  10,  11,  ...,  14,  16,  26],\n",
       "          [ 27,  28,  29,  ...,  32,  33,  53],\n",
       "          ...,\n",
       "          [ 63,  64,  65,  ...,  68,  69,  70],\n",
       "          [ 81,  82,  83,  ..., 105, 106, 106],\n",
       "          [ 99, 100, 101,  ..., 114, 114, 106]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0,   1,  12,  ...,   5,   6,   7],\n",
       "          [  9,  10,  21,  ...,  14,  15,  16],\n",
       "          [ 27,  28,  29,  ...,  32,  33,  34],\n",
       "          ...,\n",
       "          [ 63,  65,  66,  ...,  68,  78,  79],\n",
       "          [ 81,  83,  83,  ...,  86,  87,  88],\n",
       "          [ 99, 100, 101,  ..., 104, 105, 106]],\n",
       "\n",
       "         [[  0,   1,   2,  ...,   5,   6,   7],\n",
       "          [  9,  10,  11,  ...,  14,  15,  16],\n",
       "          [ 27,  28,  29,  ...,  32,  33,  34],\n",
       "          ...,\n",
       "          [ 63,  64,  65,  ...,  86,  69,  70],\n",
       "          [ 81,  82,  83,  ...,  86,  87,  88],\n",
       "          [ 99, 100, 101,  ..., 104, 105, 106]],\n",
       "\n",
       "         [[  0,   1,   2,  ...,   5,   6,   7],\n",
       "          [  9,  10,  11,  ...,  14,  15,  16],\n",
       "          [ 27,  28,  29,  ...,  32,  33,  34],\n",
       "          ...,\n",
       "          [ 63,  64,  65,  ...,  68,  69,  70],\n",
       "          [ 81,  82,  83,  ...,  86,  87,  88],\n",
       "          [ 99, 100, 101,  ..., 104, 105, 106]]]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_pool(fts,bbox)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts=model.features[0:-1](sample_ds[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[201.7280, 117.2048, 339.9680, 333.1084,   0.0000]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([201.7280, 117.2048, 339.9680, 333.1084,   0.0000])\n"
     ]
    }
   ],
   "source": [
    "bboxs=sample_ds[1]\n",
    "for bbox in bboxs:\n",
    "    print(bbox)\n",
    "    #bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_bbox=[0]*4\n",
    "img_shape=sample_ds[0].shape[1]\n",
    "scaled_bbox[0:4]=bbox[0:4]/img_shape\n",
    "scaled_bbox=[int(x*32) for x in scaled_bbox]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_bbox[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi=fts[:,:,scaled_bbox[1]:scaled_bbox[3],scaled_bbox[0]:scaled_bbox[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (7, 7)\n",
    "adaptive_max_pool = nn.AdaptiveMaxPool2d(size[0], size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveMaxPool2d(output_size=7)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptive_max_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptive_max_pool(roi)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptive_max_pool(roi)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
