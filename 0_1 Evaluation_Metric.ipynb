{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLoad Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ipynb file consists of description of evaluation metrics used in object detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:/Dataset/Oxford\\images\\Abyssinian_1.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>333</td>\n",
       "      <td>72</td>\n",
       "      <td>425</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:/Dataset/Oxford\\images\\Abyssinian_10.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>105</td>\n",
       "      <td>288</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:/Dataset/Oxford\\images\\Abyssinian_100.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>71</td>\n",
       "      <td>335</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:/Dataset/Oxford\\images\\Abyssinian_101.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>36</td>\n",
       "      <td>319</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:/Dataset/Oxford\\images\\Abyssinian_102.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>325</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      filename  target  xmin  ymin  xmax  ymax\n",
       "0    D:/Dataset/Oxford\\images\\Abyssinian_1.jpg       0   333    72   425   158\n",
       "1   D:/Dataset/Oxford\\images\\Abyssinian_10.jpg       0    72   105   288   291\n",
       "2  D:/Dataset/Oxford\\images\\Abyssinian_100.jpg       0   151    71   335   267\n",
       "3  D:/Dataset/Oxford\\images\\Abyssinian_101.jpg       0    54    36   319   235\n",
       "4  D:/Dataset/Oxford\\images\\Abyssinian_102.jpg       0    23    27   325   320"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(precision=2)\n",
    "##File Management\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "#Image,Numpy\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#Img Augment\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "from torchvision import transforms as T\n",
    "from dataload import xml_to_csv,PetData\n",
    "import random\n",
    "root_path=\"D:/Dataset/Oxford\"\n",
    "\n",
    "img_path=os.path.join(root_path,\"images\")\n",
    "annotation_path=os.path.join(root_path,\"annotations/xmls\")           \n",
    "annots = glob.glob(annotation_path+\"/*.xml\")\n",
    "df=xml_to_csv(annots,img_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 5\n",
    "train_ds = PetData(df, train=True,tensor_return=False)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE\n",
    "                                       , collate_fn=collate_fn,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Evaluation Metrics: mAP ([Link](https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#evaluation-metrics-map))\n",
    "A common evaluation metric used in many object recognition and detection tasks is **“mAP”**, short for **“mean average precision”**. It is a number from 0 to 100; higher value is better.\n",
    "\n",
    "+ Combine all detections from all test images to draw a precision-recall curve (PR curve) for each class; The “average precision” (AP) is the area under the PR curve.\n",
    "+ Given that target objects are in different classes, we first compute AP separately for each class, and then average over classes.\n",
    "+ A detection is a true positive if it has “intersection over union” (IoU) with a ground-truth box greater than some threshold (usually 0.5; if so, the metric is “mAP@0.5”)\n",
    "\n",
    "[(Python으로 구현한 mAP)](https://herbwood.tistory.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Measuring IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_getIntersectArea(boxA,boxB):\n",
    "    #Will return none if no intersect\n",
    "    dx = min(boxA[2], boxB[2]) - max(boxA[0], boxB[0])\n",
    "    dy = min(boxA[3], boxB[3]) - max(boxA[1], boxB[1])\n",
    "\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return float(dx*dy)\n",
    "def torch_getArea(box):\n",
    "    return float((box[2] - box[0] ) * (box[3] - box[1]))\n",
    "def torch_getUnion(boxA,boxB,inter_area):\n",
    "    return torch_getArea(boxA)+torch_getArea(boxB)-inter_area\n",
    "\n",
    "def torch_getIOU(boxA,boxB):\n",
    "    I=torch_getIntersectArea(boxA,boxB)\n",
    "\n",
    "    if I is None: \n",
    "        return 0\n",
    "    U=torch_getUnion(boxA,boxB,I)\n",
    "   # return float(I)/float(U)\n",
    "\n",
    "    return torch.div(I,U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_gt_box = torch.tensor((394, 124, 429, 180))\n",
    "torch_pred_box =  torch.tensor((360, 100, 450, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drawing Boxes on Img\n",
    "def draw_box(img,boxes):\n",
    "    COLOR=['red','blue']\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    for box in  boxes:\n",
    "        ax.add_patch(\n",
    "     patches.Rectangle(\n",
    "        (box.x1,box.y1),\n",
    "        (box.x2-box.x1),\n",
    "        (box.y2-box.y1),\n",
    "        edgecolor = COLOR[box.label],\n",
    "        fill=False ) )\n",
    "    plt.show()\n",
    "draw_box(np.ones((500,500,3)),[BoundingBox(*torch_gt_box,label=0),BoundingBox(*torch_pred_box,label=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1088888868689537"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print_IOU\n",
    "\n",
    "torch_getIOU(torch_pred_box,torch_gt_box).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_gt_(data):\n",
    "   # return [random.randint(0,len(data)-1) for _ in range(3)]\n",
    "    return [data[x][1][0] for x in [random.randint(0,len(data)-1) for _ in range(5)]]\n",
    "def get_random_pd_(gts,x=60):\n",
    "    newbox_list=[]\n",
    "    conf_score_list=[]\n",
    "    for bbox in gts:\n",
    "        newx1=np.clip(bbox.x1+((random.randint(0,1)*-1)*random.randint(0,x)),0,9999)\n",
    "        newx2=np.clip(bbox.x2+((random.randint(0,1)*-1)*random.randint(0,x)),0,9999)\n",
    "        newy1=np.clip(bbox.y1+((random.randint(0,1)*-1)*random.randint(0,x)),0,9999)\n",
    "        newy2=np.clip(bbox.y2+((random.randint(0,1)*-1)*random.randint(0,x)),0,9999)\n",
    "        newbox_list.append(torch.tensor((newx1,newy1,newx2,newy2,random.randint(0,1),random.uniform(0.5,1))))\n",
    "       # newbox_list.append(torch.tensor((newx1,newy1,newx2,newy2,bbox.label,random.uniform(0.7,1))))\n",
    "    for _ in range(random.randint(0,4)):\n",
    "        i=random.randint(0,len(gts)-1)\n",
    "        newx1=np.clip(gts[i].x1+((random.randint(0,1)*-1)*random.randint(0,x)),0,9999)\n",
    "        newx2=np.clip(gts[i].x2+((random.randint(0,1)*-1)*random.randint(0,x)),0,9999)\n",
    "        newy1=np.clip(gts[i].y1+((random.randint(0,1)*-1)*random.randint(0,x)),0,9999)\n",
    "        newy2=np.clip(gts[i].y2+((random.randint(0,1)*-1)*random.randint(0,x)),0,9999)\n",
    "        newbox_list.append(torch.tensor((newx1,newx2,newx2,newy2,random.randint(0,1),random.uniform(0.5,1))))\n",
    "    return torch.stack(newbox_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Input:\n",
      "tensor([[116.48,  27.33, 223.40,  79.30,   0.00],\n",
      "        [ 40.91,  30.02, 105.29,  78.40,   0.00],\n",
      "        [ 84.22,  61.53, 149.63, 155.90,   1.00],\n",
      "        [  1.34,  16.57, 198.91, 195.19,   1.00],\n",
      "        [ 82.88,  14.80, 179.65, 158.75,   1.00]])\n",
      "tensor([[ 77.06,  28.67, 172.63, 102.14,   1.00],\n",
      "        [ 64.64,  34.50, 133.76, 138.88,   1.00],\n",
      "        [ 75.22,   3.73, 147.96,  71.68,   0.00],\n",
      "        [ 67.95,   1.00, 165.01, 150.33,   1.00],\n",
      "        [ 30.46,  29.87, 174.72, 164.27,   1.00]])\n",
      "\n",
      "Prediction Input\n",
      "\n",
      "tensor([[116.48,   0.00, 223.40,  67.30,   1.00,   0.75],\n",
      "        [  3.91,   5.02, 105.29,  49.40,   1.00,   0.52],\n",
      "        [ 45.22,  59.53, 118.63, 155.90,   0.00,   0.86],\n",
      "        [  0.00,   0.00, 175.91, 195.19,   0.00,   0.99],\n",
      "        [ 82.88,   0.00, 123.65, 158.75,   1.00,   0.86]], dtype=torch.float64)\n",
      "tensor([[ 77.06,  28.67, 172.63,  94.14,   1.00,   0.58],\n",
      "        [ 64.64,  34.50,  77.76, 138.88,   1.00,   0.69],\n",
      "        [ 39.22,   0.00, 108.96,  38.68,   0.00,   0.57],\n",
      "        [ 67.95,   0.00, 123.01,  95.33,   0.00,   0.54],\n",
      "        [ 30.46,   0.00, 133.72, 164.27,   1.00,   0.78],\n",
      "        [ 10.95, 165.01, 165.01, 109.33,   0.00,   0.70]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "labels=[0,1]\n",
    "\n",
    "gtss=[get_random_gt_(train_ds),get_random_gt_(train_ds)]           #gtss=Ground Truth BBox  in multiimages shape [Image # * BBOX # * (BBOX_INFORMATION: [0]=xmin,[1]=ymin,[2]=xmax,[3]=ymax,[4]=label\n",
    "ptss=[get_random_pd_(gtss[0]),get_random_pd_(gtss[1])] #ptss=Predicted BBox  in multiimages shape [Image # * BBOX # * (BBOX_INFORMATION: [0]=xmin,[1]=ymin,[2]=xmax,[3]=ymax,[4]=label,[5]=conf.)\n",
    "\n",
    "#CVT to tensor\n",
    "bbox_class=False\n",
    "if bbox_class:\n",
    "    gts=torch.stack(gts)\n",
    "\n",
    "print(\"Ground Truth Input:\")\n",
    "\n",
    "# print('Before Conversion')\n",
    "# for i in gtss:\n",
    "#     print(i)\n",
    "\n",
    "def cvt_bbsTOtensor(bbs):\n",
    "    return(torch.stack([torch.tensor([bb.x1,bb.y1,bb.x2,bb.y2,bb.label]) for bb in bbs]))\n",
    "\n",
    "#print('After Conversion')\n",
    "gtss=[cvt_bbsTOtensor(gt) for gt in gtss]\n",
    "for i in gtss:\n",
    "    print(i)\n",
    "\n",
    "print('\\nPrediction Input')\n",
    "print()\n",
    "for i in ptss:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_img_index = list()\n",
    "for i in range(len(gtss)):\n",
    "    true_img_index+=([i] * gtss[i].size(0))\n",
    "true_img_index=torch.tensor(true_img_index).unsqueeze(1)\n",
    "true_boxes = torch.cat(gtss, dim=0)  # (n_objects, 4)\n",
    "true_boxes=torch.cat((true_boxes.float(),true_img_index.float()),1)    \n",
    "det_img_index=list()\n",
    "for i in range(len(ptss)):\n",
    "    det_img_index+=([i] * ptss[i].size(0))\n",
    "det_img_index=torch.LongTensor(det_img_index).unsqueeze(1)\n",
    "det_boxes = torch.cat(ptss, dim=0)  # (n_objects, 4)\n",
    "det_boxes=torch.cat((det_boxes.float(),det_img_index.float()),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Precision_Recall](img/Intro/Precision_Recall.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=2\n",
    "iou_threshold=0.5\n",
    "#det_boxes = torch.stack(sorted(det_boxes, key = lambda x: (x[4], x[5]), reverse=True))\n",
    "\n",
    "\n",
    "def cal_map(true_data,pred_data,labels,debug=False):\n",
    "    ap = torch.zeros((len(labels)))\n",
    "    eval_df=None\n",
    "    for label_idx,label in enumerate(labels):\n",
    "\n",
    "        #Get detected box/ground_truth for a single label\n",
    "\n",
    "        pred_images=[p for p in pred_data if p[4]==label]\n",
    "        gths_images=[g for g in true_data if g[4]==label]\n",
    "\n",
    "        #Sore detected images by its prevision\n",
    "\n",
    "        pred_images=(sorted(pred_images, key = lambda x: (x[5]), reverse=True))\n",
    "\n",
    "        \n",
    "        #Create flag ,TP,FP for matched true bbox\n",
    "        num_gt=len(gths_images)\n",
    "        num_pd=len(pred_images)\n",
    "\n",
    "        TP=torch.zeros(num_pd)\n",
    "        FP=torch.ones(num_pd)\n",
    "        gths_check=[0]*len(gths_images)\n",
    "        def get_same_image(dets,img_idx):\n",
    "            gt=[]\n",
    "            idx_list=[]\n",
    "            for idx,det in enumerate(dets):\n",
    "                if det[-1]==img_idx:\n",
    "                    gt.append(det)\n",
    "                    idx_list.append(idx)\n",
    "\n",
    "            return gt,idx_list\n",
    "        # Matching each detected bbox of a class\n",
    "        eval_dataframe_class=[{'Class':f'{label_idx}','Image':f'Image {int(data[-1])}', \n",
    "                               'Detection':f'P{i}', 'Confidence \\%': int(data[-2].item()*100)} for i,data in enumerate(pred_images)] \n",
    "        for d_index,det in enumerate(pred_images):\n",
    "            \n",
    "            gths,check_idxs = get_same_image(gths_images,det[-1]) #get ground truths from image that belongs to same image as detected box\n",
    "\n",
    "            \n",
    "            maxIou=torch.tensor(0)\n",
    "            #Find Maximum matching iou box\n",
    "            for gt,check_idx in zip(gths,check_idxs):\n",
    "\n",
    "                iou=torch_getIOU(det[0:4],gt[0:4])\n",
    "               # print(gt)\n",
    "               # print(det)\n",
    "               # draw_box(np.ones((224,224,3)),\n",
    "               #          [BoundingBox(det[0:4],label=0)]+[BoundingBox(*ground[0:4],label=1) for ground in gths])\n",
    "                if iou > maxIou:\n",
    "                    maxIou = iou\n",
    "                    erase_idx = check_idx\n",
    "                           \n",
    "            if maxIou >=iou_threshold:        # If iou> threshold \n",
    "                eval_dataframe_class[d_index]['ioU']='{:.2f}'.format(maxIou.item())\n",
    "                #f'>{iou_threshold}'\n",
    "                if gths_check[erase_idx]==0:  # and if unmatched yet\n",
    "                    TP[d_index]=1             # the bbox is true positive \n",
    "                    gths_check[erase_idx]=1   # flag gt_box as matched \n",
    "                    eval_dataframe_class[d_index]['Ground Truth']=f'GT{erase_idx}'\n",
    "                    eval_dataframe_class[d_index]['TP/FP']=\"TP\"\n",
    "                else:                         # else <threshold if already matched, it is false positive\n",
    "                    eval_dataframe_class[d_index]['Ground Truth']=f'GT{erase_idx}'\n",
    "                    eval_dataframe_class[d_index]['TP/FP']=\"FP\"\n",
    "\n",
    "            else:\n",
    "                eval_dataframe_class[d_index]['ioU']='{:.2f}'.format(maxIou.item())\n",
    "                eval_dataframe_class[d_index]['Ground Truth']=f'-'\n",
    "                eval_dataframe_class[d_index]['TP/FP']=\"FP\"\n",
    "                \n",
    "        FP=FP-TP\n",
    "        acc_FP = torch.cumsum(FP, dim=0)\n",
    "        acc_TP = torch.cumsum(TP, dim=0)\n",
    "        rec = acc_TP / (num_gt+ 1e-10)\n",
    "        prec = (acc_TP/ (acc_FP + acc_TP+1e-10)) #Prevent Division by zero\n",
    "        \n",
    "        eval_dataframe_class=pd.DataFrame.from_dict(eval_dataframe_class).join(pd.DataFrame({\"Acc TP\":acc_TP.tolist(),\n",
    "                                                            \"Acc FP\":acc_FP.tolist(),\n",
    "                                                            \"Precision\":prec.tolist(),\n",
    "                                                            \"Recall\":rec.tolist()}))\n",
    "\n",
    "        \n",
    "        ##11-point https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection\n",
    "        recall_thresholds = torch.arange(start=0, end=1.1, step=.1)\n",
    "        precisions = torch.zeros((len(recall_thresholds)))\n",
    "        for i, t in enumerate(recall_thresholds):\n",
    "            recalls_above_t = rec >= t\n",
    "            if recalls_above_t.any():\n",
    "                precisions[i] = prec[recalls_above_t].max()\n",
    "            else:\n",
    "                precisions[i] = 0.\n",
    "      #  plt.figure()        \n",
    "       # plt.plot(recall_thresholds,precisions)\n",
    "        ap[label_idx] = precisions.mean()  \n",
    "        \n",
    "        if eval_df is None:\n",
    "            eval_df=eval_dataframe_class\n",
    "        else:\n",
    "            eval_df = pd.concat([eval_df,eval_dataframe_class],ignore_index=True)\n",
    "    return ap,eval_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Image</th>\n",
       "      <th>Detection</th>\n",
       "      <th>Confidence \\%</th>\n",
       "      <th>ioU</th>\n",
       "      <th>Ground Truth</th>\n",
       "      <th>TP/FP</th>\n",
       "      <th>Acc TP</th>\n",
       "      <th>Acc FP</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Image 0</td>\n",
       "      <td>P0</td>\n",
       "      <td>99</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Image 0</td>\n",
       "      <td>P1</td>\n",
       "      <td>86</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Image 1</td>\n",
       "      <td>P2</td>\n",
       "      <td>69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Image 1</td>\n",
       "      <td>P3</td>\n",
       "      <td>57</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Image 1</td>\n",
       "      <td>P4</td>\n",
       "      <td>53</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>Image 0</td>\n",
       "      <td>P0</td>\n",
       "      <td>86</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-</td>\n",
       "      <td>FP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Image 1</td>\n",
       "      <td>P1</td>\n",
       "      <td>78</td>\n",
       "      <td>0.62</td>\n",
       "      <td>GT6</td>\n",
       "      <td>TP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Image 0</td>\n",
       "      <td>P2</td>\n",
       "      <td>74</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-</td>\n",
       "      <td>FP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Image 1</td>\n",
       "      <td>P3</td>\n",
       "      <td>69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-</td>\n",
       "      <td>FP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Image 1</td>\n",
       "      <td>P4</td>\n",
       "      <td>58</td>\n",
       "      <td>0.89</td>\n",
       "      <td>GT3</td>\n",
       "      <td>TP</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>Image 0</td>\n",
       "      <td>P5</td>\n",
       "      <td>51</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-</td>\n",
       "      <td>FP</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class    Image Detection  Confidence \\%   ioU Ground Truth TP/FP  Acc TP  \\\n",
       "0      0  Image 0        P0             99  0.09            -    FP     0.0   \n",
       "1      0  Image 0        P1             86  0.13            -    FP     0.0   \n",
       "2      0  Image 1        P2             69  0.00            -    FP     0.0   \n",
       "3      0  Image 1        P3             57  0.18            -    FP     0.0   \n",
       "4      0  Image 1        P4             53  0.47            -    FP     0.0   \n",
       "5      1  Image 0        P0             86  0.42            -    FP     0.0   \n",
       "6      1  Image 1        P1             78  0.62          GT6    TP     1.0   \n",
       "7      1  Image 0        P2             74  0.19            -    FP     1.0   \n",
       "8      1  Image 1        P3             69  0.19            -    FP     1.0   \n",
       "9      1  Image 1        P4             58  0.89          GT3    TP     2.0   \n",
       "10     1  Image 0        P5             51  0.09            -    FP     2.0   \n",
       "\n",
       "    Acc FP  Precision    Recall  \n",
       "0      1.0   0.000000  0.000000  \n",
       "1      2.0   0.000000  0.000000  \n",
       "2      3.0   0.000000  0.000000  \n",
       "3      4.0   0.000000  0.000000  \n",
       "4      5.0   0.000000  0.000000  \n",
       "5      1.0   0.000000  0.000000  \n",
       "6      1.0   0.500000  0.142857  \n",
       "7      2.0   0.333333  0.142857  \n",
       "8      3.0   0.250000  0.142857  \n",
       "9      3.0   0.400000  0.285714  \n",
       "10     4.0   0.333333  0.285714  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AP_class,eval_dataframe=cal_map(true_boxes,det_boxes,labels)\n",
    "eval_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
