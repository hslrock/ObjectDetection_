{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLoad Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ipynb file consists of description of evaluation metrics used in object detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:/Dataset/Pet_Data/images\\Abyssinian_1.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>333</td>\n",
       "      <td>72</td>\n",
       "      <td>425</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:/Dataset/Pet_Data/images\\Abyssinian_10.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>105</td>\n",
       "      <td>288</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:/Dataset/Pet_Data/images\\Abyssinian_100.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>71</td>\n",
       "      <td>335</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:/Dataset/Pet_Data/images\\Abyssinian_101.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>36</td>\n",
       "      <td>319</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:/Dataset/Pet_Data/images\\Abyssinian_102.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>325</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        filename  target  xmin  ymin  xmax  \\\n",
       "0    D:/Dataset/Pet_Data/images\\Abyssinian_1.jpg       0   333    72   425   \n",
       "1   D:/Dataset/Pet_Data/images\\Abyssinian_10.jpg       0    72   105   288   \n",
       "2  D:/Dataset/Pet_Data/images\\Abyssinian_100.jpg       0   151    71   335   \n",
       "3  D:/Dataset/Pet_Data/images\\Abyssinian_101.jpg       0    54    36   319   \n",
       "4  D:/Dataset/Pet_Data/images\\Abyssinian_102.jpg       0    23    27   325   \n",
       "\n",
       "   ymax  \n",
       "0   158  \n",
       "1   291  \n",
       "2   267  \n",
       "3   235  \n",
       "4   320  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(precision=2)\n",
    "##File Management\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "#Image,Numpy\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#Img Augment\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "from torchvision import transforms as T\n",
    "from dataload import xml_to_csv,PetData\n",
    "import random\n",
    "root_path=\"D:/Dataset/Pet_Data/\"\n",
    "img_path=os.path.join(root_path,\"images\")\n",
    "annotation_path=os.path.join(root_path,\"annotations/xmls\")           \n",
    "annots = glob.glob(annotation_path+\"/*.xml\")\n",
    "df=xml_to_csv(annots,img_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 5\n",
    "train_ds = PetData(df, train=True,tensor_return=False)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE\n",
    "                                       , collate_fn=collate_fn,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Evaluation Metrics: mAP ([Link](https://lilianweng.github.io/lil-log/2017/12/15/object-recognition-for-dummies-part-2.html#evaluation-metrics-map))\n",
    "A common evaluation metric used in many object recognition and detection tasks is **“mAP”**, short for **“mean average precision”**. It is a number from 0 to 100; higher value is better.\n",
    "\n",
    "+ Combine all detections from all test images to draw a precision-recall curve (PR curve) for each class; The “average precision” (AP) is the area under the PR curve.\n",
    "+ Given that target objects are in different classes, we first compute AP separately for each class, and then average over classes.\n",
    "+ A detection is a true positive if it has “intersection over union” (IoU) with a ground-truth box greater than some threshold (usually 0.5; if so, the metric is “mAP@0.5”)\n",
    "\n",
    "[(Python으로 구현한 mAP)](https://herbwood.tistory.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Measuring IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_getIntersectArea(boxA,boxB):\n",
    "    #Will return none if no intersect\n",
    "    dx = min(boxA[2], boxB[2]) - max(boxA[0], boxB[0])\n",
    "    dy = min(boxA[3], boxB[3]) - max(boxA[1], boxB[1])\n",
    "\n",
    "    if (dx>=0) and (dy>=0):\n",
    "        return float(dx*dy)\n",
    "def torch_getArea(box):\n",
    "    return float((box[2] - box[0] ) * (box[3] - box[1]))\n",
    "def torch_getUnion(boxA,boxB,inter_area):\n",
    "    return torch_getArea(boxA)+torch_getArea(boxB)-inter_area\n",
    "\n",
    "def torch_getIOU(boxA,boxB):\n",
    "    I=torch_getIntersectArea(boxA,boxB)\n",
    "\n",
    "    if I is None: \n",
    "        return 0\n",
    "    U=torch_getUnion(boxA,boxB,I)\n",
    "   # return float(I)/float(U)\n",
    "\n",
    "    return torch.div(I,U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_gt_box = torch.tensor((394, 124, 429, 180))\n",
    "torch_pred_box =  torch.tensor((380, 120, 420, 170))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drawing Boxes on Img\n",
    "def draw_box(img,boxes):\n",
    "    COLOR=['red','blue']\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(img)\n",
    "    for box in  boxes:\n",
    "        ax.add_patch(\n",
    "     patches.Rectangle(\n",
    "        (box.x1,box.y1),\n",
    "        (box.x2-box.x1),\n",
    "        (box.y2-box.y1),\n",
    "        edgecolor = COLOR[box.label],\n",
    "        fill=False ) )\n",
    "    plt.show()\n",
    "draw_box(np.ones((500,500,3)),[BoundingBox(*torch_gt_box,label=0),BoundingBox(*torch_pred_box,label=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43270623683929443"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print_IOU\n",
    "\n",
    "torch_getIOU(torch_pred_box,torch_gt_box).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_gt_(data):\n",
    "   # return [random.randint(0,len(data)-1) for _ in range(3)]\n",
    "    return [data[x][1][0] for x in [random.randint(0,len(data)-1) for _ in range(7)]]\n",
    "def get_random_pd_(gts,x=5):\n",
    "    newbox_list=[]\n",
    "    conf_score_list=[]\n",
    "    for bbox in gts:\n",
    "        newx1=bbox.x1#+((random.randint(0,1)*-1)*random.randint(0,x))\n",
    "        newx2=bbox.x2#+((random.randint(0,1)*-1)*random.randint(0,x))\n",
    "        newy1=bbox.y1#+((random.randint(0,1)*-1)*random.randint(0,x))\n",
    "        newy2=bbox.y2#+((random.randint(0,1)*-1)*random.randint(0,x))\n",
    "        #newbox_list.append(torch.tensor((newx1,newy1,newx2,newy2,random.randint(0,1),random.uniform(0.5,1))))\n",
    "        newbox_list.append(torch.tensor((newx1,newy1,newx2,newy2,bbox.label,random.uniform(0.7,1))))\n",
    "#     for _ in range(random.randint(0,3)):\n",
    "#         i=random.randint(0,len(gts)-1)\n",
    "#         newx1=gts[i].x1+((random.randint(0,1)*-1)*random.randint(0,x))\n",
    "#         newx2=gts[i].x2+((random.randint(0,1)*-1)*random.randint(0,x))\n",
    "#         newy1=gts[i].y1+((random.randint(0,1)*-1)*random.randint(0,x))\n",
    "#         newy2=gts[i].y2+((random.randint(0,1)*-1)*random.randint(0,x))\n",
    "#         newbox_list.append(torch.tensor((newx1,newx2,newx2,newy2,random.randint(0,1),random.uniform(0.5,1))))\n",
    "    return torch.stack(newbox_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth Input:\n",
      "tensor([[134.96,  26.01, 173.60,  85.57,   0.00],\n",
      "        [ 19.31,  10.47,  84.19, 108.86,   1.00],\n",
      "        [ 50.30,  25.54, 153.58, 141.12,   1.00],\n",
      "        [ 50.60,  34.50, 184.19, 130.82,   0.00],\n",
      "        [ 61.82,  19.11, 159.94, 206.08,   1.00],\n",
      "        [ 91.48,  17.92, 158.75,  58.24,   1.00],\n",
      "        [ 52.27,  99.56, 127.68, 190.15,   1.00]])\n",
      "tensor([[ 43.85,  45.70, 169.48, 120.06,   1.00],\n",
      "        [ 36.09,   9.55, 100.80,  98.41,   0.00],\n",
      "        [ 19.26,   0.67, 181.44, 178.26,   0.00],\n",
      "        [ 64.21,   1.00, 139.63, 106.52,   1.00],\n",
      "        [ 43.46,  11.44, 103.49, 141.26,   1.00],\n",
      "        [ 81.39,  37.83, 135.89, 108.52,   1.00],\n",
      "        [ 80.04,  44.80, 159.49, 106.62,   1.00]])\n",
      "\n",
      "Prediction Input\n",
      "tensor([[134.96,  26.01, 173.60,  85.57,   0.00,   0.74],\n",
      "        [ 19.31,  10.47,  84.19, 108.86,   1.00,   0.70],\n",
      "        [ 50.30,  25.54, 153.58, 141.12,   1.00,   0.82],\n",
      "        [ 50.60,  34.50, 184.19, 130.82,   0.00,   0.95],\n",
      "        [ 61.82,  19.11, 159.94, 206.08,   1.00,   0.87],\n",
      "        [ 91.48,  17.92, 158.75,  58.24,   1.00,   0.94],\n",
      "        [ 52.27,  99.56, 127.68, 190.15,   1.00,   0.87]])\n",
      "tensor([[ 43.85,  45.70, 169.48, 120.06,   1.00,   0.82],\n",
      "        [ 36.09,   9.55, 100.80,  98.41,   0.00,   0.72],\n",
      "        [ 19.26,   0.67, 181.44, 178.26,   0.00,   0.76],\n",
      "        [ 64.21,   1.00, 139.63, 106.52,   1.00,   0.81],\n",
      "        [ 43.46,  11.44, 103.49, 141.26,   1.00,   0.74],\n",
      "        [ 81.39,  37.83, 135.89, 108.52,   1.00,   0.74],\n",
      "        [ 80.04,  44.80, 159.49, 106.62,   1.00,   0.89]])\n"
     ]
    }
   ],
   "source": [
    "labels=[0,1]\n",
    "\n",
    "gtss=[get_random_gt_(train_ds),get_random_gt_(train_ds)]           #gtss=Ground Truth BBox  in multiimages shape [Image # * BBOX # * (BBOX_INFORMATION: [0]=xmin,[1]=ymin,[2]=xmax,[3]=ymax,[4]=label\n",
    "ptss=[get_random_pd_(gtss[0]),get_random_pd_(gtss[1])] #ptss=Predicted BBox  in multiimages shape [Image # * BBOX # * (BBOX_INFORMATION: [0]=xmin,[1]=ymin,[2]=xmax,[3]=ymax,[4]=label,[5]=conf.)\n",
    "\n",
    "#CVT to tensor\n",
    "bbox_class=False\n",
    "if bbox_class:\n",
    "    gts=torch.stack(gts)\n",
    "\n",
    "print(\"Ground Truth Input:\")\n",
    "\n",
    "# print('Before Conversion')\n",
    "# for i in gtss:\n",
    "#     print(i)\n",
    "\n",
    "def cvt_bbsTOtensor(bbs):\n",
    "    return(torch.stack([torch.tensor([bb.x1,bb.y1,bb.x2,bb.y2,bb.label]) for bb in bbs]))\n",
    "\n",
    "#print('After Conversion')\n",
    "gtss=[cvt_bbsTOtensor(gt) for gt in gtss]\n",
    "for i in gtss:\n",
    "    print(i)\n",
    "\n",
    "print('\\nPrediction Input')\n",
    "\n",
    "for i in ptss:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_img_index = list()\n",
    "for i in range(len(gtss)):\n",
    "    true_img_index+=([i] * gtss[i].size(0))\n",
    "true_img_index=torch.tensor(true_img_index).unsqueeze(1)\n",
    "true_boxes = torch.cat(gtss, dim=0)  # (n_objects, 4)\n",
    "true_boxes=torch.cat((true_boxes,true_img_index.float()),1)    \n",
    "det_img_index=list()\n",
    "for i in range(len(ptss)):\n",
    "    det_img_index+=([i] * ptss[i].size(0))\n",
    "det_img_index=torch.LongTensor(det_img_index).unsqueeze(1)\n",
    "det_boxes = torch.cat(ptss, dim=0)  # (n_objects, 4)\n",
    "det_boxes=torch.cat((det_boxes,det_img_index.float()),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Precision_Recall](img/Intro/Precision_Recall.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize=2\n",
    "iou_threshold=0.8\n",
    "#det_boxes = torch.stack(sorted(det_boxes, key = lambda x: (x[4], x[5]), reverse=True))\n",
    "\n",
    "\n",
    "def cal_map(true_data,pred_data,labels,debug=False):\n",
    "    ap = torch.zeros((len(labels)))\n",
    "    for label_idx,label in enumerate(labels):\n",
    "\n",
    "        #Get detected box/ground_truth for a single label\n",
    "\n",
    "        pred_images=[p for p in det_boxes if p[4]==label]\n",
    "        gths_images=[g for g in pred_data if g[4]==label]\n",
    "\n",
    "        #Sore detected images by its prevision\n",
    "\n",
    "        pred_images=(sorted(pred_images, key = lambda x: (x[5]), reverse=True))\n",
    "\n",
    "        #Create flag ,TP,FP for matched true bbox\n",
    "        num_gt=len(gths_images)\n",
    "        num_pd=len(pred_images)\n",
    "        TP=torch.zeros(num_pd)\n",
    "        FP=torch.zeros(num_pd)\n",
    "        gths_check=[0]*len(gths_images)\n",
    "\n",
    "        def get_same_image(dets,img_idx):\n",
    "            gt=[]\n",
    "            idx_list=[]\n",
    "            for idx,det in enumerate(dets):\n",
    "                if det[-1]==img_idx:\n",
    "                    gt.append(det)\n",
    "                    idx_list.append(idx)\n",
    "\n",
    "            return gt,idx_list\n",
    "        # Matching each detected bbox of a class\n",
    "        for d_index,det in enumerate(pred_images):\n",
    "            gths,check_idxs = get_same_image(gths_images,det[-1]) #get ground truths from image that belongs to same image as detected box\n",
    "\n",
    "            \n",
    "            maxIou=0\n",
    "            #Find Maximum matching iou box\n",
    "            for gt,check_idx in zip(gths,check_idxs):\n",
    "                iou=torch_getIOU(det[0:4],gt[0:4])\n",
    "                if iou > maxIou:\n",
    "                    maxIou = iou\n",
    "                    erase_idx = check_idx\n",
    "                if maxIou >=iou_threshold:        # If iou> threshold \n",
    "                    if gths_check[erase_idx]==0:  # and if unmatched yet\n",
    "                        TP[d_index]=1             # the bbox is true positive \n",
    "                        gths_check[erase_idx]=1   # flag gt_box as matched \n",
    "                    else:                         # else <threshold if already matched, it is false positive\n",
    "                         FP[d_index] = 1\n",
    "                else:\n",
    "                    FP[d_index]=1\n",
    "\n",
    "        acc_FP = torch.cumsum(FP, dim=0)\n",
    "        acc_TP = torch.cumsum(TP, dim=0)\n",
    "        rec = acc_TP / (num_gt+ 1e-10)\n",
    "        prec = (acc_TP/ (acc_FP + acc_TP+1e-10)) #Prevent Division by zero\n",
    "\n",
    "        ##11-point https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection\n",
    "        recall_thresholds = torch.arange(start=0, end=1.1, step=.1)\n",
    "        precisions = torch.zeros((len(recall_thresholds)))\n",
    "        for i, t in enumerate(recall_thresholds):\n",
    "            recalls_above_t = rec >= t\n",
    "            if recalls_above_t.any():\n",
    "                precisions[i] = prec[recalls_above_t].max()\n",
    "            else:\n",
    "                precisions[i] = 0.\n",
    "      #  plt.figure()        \n",
    "       # plt.plot(recall_thresholds,precisions)\n",
    "        ap[label_idx] = precisions.mean()  \n",
    "    return ap\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP Per Class: \n",
      "tensor([0.50, 0.50])\n",
      "mAP\n",
      "tensor(0.50)\n"
     ]
    }
   ],
   "source": [
    "AP_class=cal_map(true_boxes,det_boxes,labels)\n",
    "print(\"AP Per Class: \")\n",
    "print(AP_class)\n",
    "print(\"mAP\")\n",
    "print(torch.mean(AP_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
