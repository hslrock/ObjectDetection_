{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make R-CNN faster, Girshick (2015) improved the training procedure by unifying three independent models into one jointly trained framework and increasing shared computation results, named Fast R-CNN. Instead of extracting CNN feature vectors independently for each region proposal, this model aggregates them into one CNN forward pass over the entire image and the region proposals share this feature matrix. Then the same feature matrix is branched out to be used for learning the object classifier and the bounding-box regressor. In conclusion, computation sharing speeds up R-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#First, pre-train a convolutional neural network on image classification tasks.\n",
    "#Propose regions by selective search (~2k candidates per image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from dataload import xml_to_csv,PetData,Sub_region_train,Sub_region\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utills import ssearch,misc\n",
    "from utills.misc import create_label,balance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading data\n",
    "#root_path=\"D:/Dataset/Pet_Data/\"\n",
    "root_path=\"D:/Dataset/Pet_Data/\"\n",
    "#root_path=\"D:/Dataset/Oxford\"\n",
    "\n",
    "img_path=os.path.join(root_path,\"images\")\n",
    "annotation_path=os.path.join(root_path,\"annotations/xmls\")           \n",
    "annots = glob.glob(annotation_path+\"/*.xml\")\n",
    "seed=0\n",
    "df=xml_to_csv(annots,img_path)\n",
    "df.head()\n",
    "\n",
    "## Make Balanced Dataset (To save time, but don't do this in real research!!)\n",
    "g = df.groupby('target')\n",
    "balanced_df = pd.DataFrame(g.apply(lambda x: x.sample(g.size().min(),random_state=seed).reset_index(drop=True),))  \n",
    "## \n",
    "train, valid = train_test_split(balanced_df, test_size=0.3,random_state=seed)  \n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updated Dataset Class (Added Selective Search \"inside\" original class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetData(Dataset):\n",
    "    def __init__(self, dataframe,train=False,ssearch=False,samples=64):\n",
    "        self.df=dataframe\n",
    "        self.ssearch=ssearch\n",
    "        self.transform=iaa.Sequential([iaa.Resize((512,512))])\n",
    "        self.torch_transform=T.Compose([T.ToTensor(),\n",
    "                                        T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])    \n",
    "        self.samples=samples\n",
    "        self.train=train\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        regions=None\n",
    "        fn,target,xmin,ymin,xmax,ymax=self.df.iloc[idx] #\n",
    "        im=cv2.cvtColor(cv2.imread(fn),cv2.COLOR_BGR2RGB) ##Load Img\n",
    "        \n",
    "        class_label=target  ##Class\n",
    "        bbs=BoundingBoxesOnImage([BoundingBox(xmin,ymin,xmax,ymax,label=class_label)], shape=im.shape) #BBox\n",
    "        image_aug, bbs_aug = self.transform(image=im, bounding_boxes=bbs) #Transformation\n",
    "        region_np=[]\n",
    "                                        \n",
    "        if self.ssearch:                                \n",
    "            regions=ssearch.selective_search(image_aug, scale=100, sigma=0.8, min_size=30)\n",
    "            regions=[dict(t) for t in {tuple(d.items()) for d in regions}]\n",
    "            \n",
    "            for dicts in regions:\n",
    "                region_np.append((np.array(dicts['rect'],dtype=np.float)))\n",
    "            region_np=torch.from_numpy(np.stack(region_np))\n",
    "       #     regions=pd.DataFrame.from_dict(regions)\n",
    "        #    regions=regions.drop_duplicates(subset=['rect'])\n",
    "          #  if self.train:\n",
    "          #      regions=regions.sample(n=self.samples)\n",
    "        #    regions=regions.reset_index()\n",
    "           # regions=regions['rect']\n",
    "         #   regions=regions.nu\n",
    "\n",
    "        return self.torch_transform(image_aug), torch.stack([torch.tensor([bb.x1,bb.y1,bb.x2,bb.y2,bb.label]) for bb in bbs_aug]),region_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PetData(train, train=True, ssearch=True)\n",
    "valid_ds= PetData(valid, train=True,ssearch=True)\n",
    "\n",
    "BATCH_SIZE=2\n",
    "def collate_fn(batch):\n",
    "    return zip(*batch)\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE\n",
    "                                       , collate_fn=collate_fn,shuffle=False)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE\n",
    "                                       , collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds=train_ds[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>    First, pre-train a convolutional neural network on image classification tasks. </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of parameters: 138357544\n"
     ]
    }
   ],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "                              \n",
    "print(f'Num of parameters: {count_parameters(model)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch=next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,bbx,regions=single_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>    We replace Last Maxpooling with ROI pooling. </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class roi_pooling(nn.Module):\n",
    "    def __init__(self,ft_shape=32,img_shape=512,size=(7,7)):\n",
    "        '''\n",
    "        '''\n",
    "        super(roi_pooling, self).__init__()\n",
    "        self.size=size\n",
    "    \n",
    "        #Layers\n",
    "        self.img_shape=img_shape\n",
    "        self.ft_shape=ft_shape\n",
    "        self.adaptivepool=nn.AdaptiveMaxPool2d(size[0], size[1])\n",
    "\n",
    "    \n",
    "    def forward(self,ft,rois):\n",
    "        batch_size=len(rois)\n",
    "        out=[]\n",
    "        for i in range(batch_size):\n",
    "            roi=rois[i]\n",
    "            ft_img=ft[i].unsqueeze(0)\n",
    "            x1=np.floor((rois[:,0]/self.img_shape)*self.ft_shape).type(torch.int32)\n",
    "            y1=np.floor((rois[:,1]/self.img_shape)*self.ft_shape).type(torch.int32)\n",
    "            x2=np.floor((rois[:,2]/self.img_shape)*self.ft_shape).type(torch.int32)\n",
    "            y2=np.floor((rois[:,3]/self.img_shape)*self.ft_shape).type(torch.int32)\n",
    "            for j in range(roi.shape[0]):\n",
    "                ft_img=ft_img[:,:,y1[j]:y2[j], x1[j]:x2[j]]\n",
    "                ft_img = self.adaptivepool(ft_img)\n",
    "                out.append(ft_img)\n",
    "        out = torch.cat(out, dim=0)  \n",
    "        return out\n",
    "        return roi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fast_RCNN(nn.Module):\n",
    "    def __init__(self,ft_shape=32,img_shape=512):\n",
    "        '''\n",
    "        '''\n",
    "        super(Fast_RCNN, self).__init__()\n",
    "    \n",
    "    \n",
    "        #Layers\n",
    "        self.ft_net=models.vgg16(pretrained=True).features[0:-1]\n",
    "        self.roi=roi_pooling()\n",
    "\n",
    "\n",
    "    def forward(self,imgs,regions):\n",
    "        \n",
    "        scaled_bbox=[0]*4\n",
    "        scaled_bbox[0:4]=region_box[0:4]/self.img_shape\n",
    "        scaled_bbox=[int(x*self.ft_shape) for x in scaled_bbox]\n",
    "        roi=ft[:,:,scaled_bbox[1]:scaled_bbox[3],scaled_bbox[0]:scaled_bbox[2]]\n",
    "        roi=self.adaptivepool(roi)[0]\n",
    "        return roi\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_pool=roi_pooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_variable=torch.stack(list(img), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts=models.vgg16(pretrained=True).features[0:-1](img_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[332., 454., 482., 511.],\n",
       "         [372., 278., 387., 335.],\n",
       "         [204., 122., 236., 192.],\n",
       "         ...,\n",
       "         [332., 454., 511., 511.],\n",
       "         [138., 494., 149., 500.],\n",
       "         [145., 292., 201., 342.]], dtype=torch.float64),\n",
       " tensor([[117.,   9., 126.,  38.],\n",
       "         [362., 177., 372., 227.],\n",
       "         [115.,  15., 115.,  60.],\n",
       "         ...,\n",
       "         [223., 214., 237., 234.],\n",
       "         [168., 400., 189., 435.],\n",
       "         [122., 458., 256., 468.]], dtype=torch.float64))"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois=regions[0]\n",
    "batch_size=len(rois)\n",
    "img_shape=512\n",
    "ft_shape=32\n",
    "x1=np.floor((rois[:,0]/img_shape)*ft_shape).type(torch.int32)\n",
    "y1=np.floor((rois[:,1]/img_shape)*ft_shape).type(torch.int32)\n",
    "x2=np.floor((rois[:,2]/img_shape)*ft_shape).type(torch.int32)\n",
    "y2=np.floor((rois[:,3]/img_shape)*ft_shape).type(torch.int32)\n",
    "out=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(rois[:,0]/img_shape*ft_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "fts=model.features[0:-1](sample_ds[0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[201.7280, 117.2048, 339.9680, 333.1084,   0.0000]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([201.7280, 117.2048, 339.9680, 333.1084,   0.0000])\n"
     ]
    }
   ],
   "source": [
    "bboxs=sample_ds[1]\n",
    "for bbox in bboxs:\n",
    "    print(bbox)\n",
    "    #bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_bbox=[0]*4\n",
    "img_shape=sample_ds[0].shape[1]\n",
    "scaled_bbox[0:4]=bbox[0:4]/img_shape\n",
    "scaled_bbox=[int(x*32) for x in scaled_bbox]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_bbox[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi=fts[:,:,scaled_bbox[1]:scaled_bbox[3],scaled_bbox[0]:scaled_bbox[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (7, 7)\n",
    "adaptive_max_pool = nn.AdaptiveMaxPool2d(size[0], size[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptiveMaxPool2d(output_size=7)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptive_max_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptive_max_pool(roi)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 7, 7])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaptive_max_pool(roi)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
